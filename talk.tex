\documentclass[xcolor={dvipsnames,table}]{beamer}
%\documentclass[handout]{beamer}

\geometry{paperwidth=190mm,paperheight=118.75mm}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
%\usetheme{Singapore}
\usecolortheme{default}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{nicefrac}
\usepackage{pifont}
\usepackage{braket}
\usepackage{colortbl}
%\usepackage{wrapfig}
%\usepackage{cutwin}
\usepackage{background}
\backgroundsetup{
    %placement=bottom,
    position={5.5,-0.3},
    scale=2.5,
    angle=0,
    pages=some,
    contents={MI250 PRELIMINARY}
}
\usetikzlibrary{arrows,shapes}
\urlstyle{same}
\definecolor{mygreen}{rgb}{0, 0.4, 0}
\usepackage{cancel}
\usepackage[detect-none]{siunitx}
\sisetup{range-phrase = \text{--}}

\usepackage[most]{tcolorbox}

\input{commands}

%compact itemize, perfect for talks
\setlength{\leftmargin}{0pt}
\setlength{\leftmargini}{0.4cm}
\setlength{\leftmarginii}{0.2cm}

\usefonttheme{professionalfonts} 

\newcommand{\roundbox}[1]{%
\noindent\begin{tikzpicture}%
  \draw node[draw=lightgray,fill=white,rounded corners,inner sep=1ex] {%
  #1
  };%
\end{tikzpicture}%
}%

\definecolor{DarkBlue}{HTML}{002e77}

\usetheme{Boadilla}
\usecolortheme[named=DarkBlue]{structure}
\useinnertheme{rounded}

\graphicspath{{graphics/}}

\title[tmLQCD + QUDA]{Twisted mass ensemble generation on GPU machines}
\author[M.~Garofalo / B.~Kostrzewa]{M.~Garofalo, B.~Kostrzewa, S.~Romiti, C.~Urbach \\ S.~Bacchio, J.~Finkenrath, F.~Pittler}
\institute[HISKP / HPC/A-Lab, Bonn~U.]{Helmholtz-Institut f√ºr Strahlen- und Kernphysik (HISKP) \&\\High Performance Computing \& Analytics Lab (HPC/A-Lab),\\ Rheinische Friedrich-Wilhelms-Universit{\"a}t Bonn}
\date[February 8th, ETMC meeting 2023 Bern]{February 8th, ETMC meeting 2023, Bern}
\titlegraphic{\roundbox{\includegraphics[height=2.5cm]{uni_bonn_hpca_hiskp_CyI_etmc}}}
\subject{Twisted mass ensemble generation on GPU machines (using tmLQCD and the QUDA library)}
\keywords{Lattice QCD, HMC, GPU, QUDA, simulation, tmLQCD}

\setbeamertemplate{navigation symbols}{}

\begin{document}

\maketitle

%\begin{frame}{Ensemble Overview}{$N_f=2+1+1$ twisted-clover}
%                                                                                                                                           
%  \includegraphics[height=70mm]{ensembles_L_vs_mpi}
%  \includegraphics[height=70mm]{ensembles_asquared_mpi}
%  \includegraphics[height=70mm]{ensembles_phys_point}
%  \begin{hpcablock}{}
%    \begin{itemize}
%      \item $N_f = 2+1+1$ twisted-clover with $c_\text{SW} \sim 1 + 0.113(3) \dfrac{g_0^2}{\langle P \rangle}$ and strange and charm quark masses tuned to within a few \% of the physical point.
%    \hfill \textcolor{blue}{[\footnotesize{\href{https://doi.org/10.1103/PhysRevD.98.054518}{Phys.Rev.D 98 (2018) 5, 054518}, \href{https://doi.org/10.22323/1.396.0284}{arXiv:2201.02551}}]}
%    \vspace{0.2cm}
%      \item $S_g = \frac{\beta}{3} \sum\limits_{x;P} \Big[ b_0 \lbrace 1 -  \text{ReTr} P^{1\times1}(x) \rbrace
%                                       + b_1 \lbrace 1 -  \text{ReTr} P^{1\times2    }(x) \rbrace \Big]$ with 
%      $b_0 = 1 - 8b_1, \, b_1=-0.331$ \hfill {\footnotesize{[Iwasaki; 1983]}}
%    \end{itemize}
%  \end{hpcablock}
%
%\end{frame}

\begin{frame}{Ensembles generation (at phys. $M_\pi$ on CPU machines)}
 	\begin{columns}
 		\begin{column}{0.5\textwidth}
 	\includegraphics[height=70mm,clip,trim = 46mm 0 0 0]{ensembles_asquared_mpi}
 	\includegraphics[height=70mm]{ensembles_phys_point}
 		\end{column}
 		\begin{column}{0.5\textwidth}
 			\begin{hpcablock}{}
 				\begin{itemize}
 				%	\item State-of-the-art \textcolor{red}{integrator} \& \textcolor{blue}{solvers} $\rightarrow$ cost scales like $(L/a)^{9/2}$ at (roughly) constant acceptance
 					\vspace{0.19cm}
 					\item need several further ensembles at larger $M_\pi \cdot L$ 
 					\vspace{0.19cm}
 					\begin{itemize}
 						\item both at the finest and the coarsest lattice spacings
 						\begin{itemize}
 							\item more statistics needed due to autocorrelations (critical slowing down and pion mass splitting)
 						\end{itemize}
 					\end{itemize}
 					%\item \textcolor{red}{cost $\mathcal{O}(10^{9})$ core-hours} \& real time per trajectory $\geq$ 6 hours at this stage
 					\vspace{0.19cm}
 					\item \textbf{Need GPU implementations}
 				\end{itemize}
 			\end{hpcablock}
 		\end{column}
 	\end{columns}
  
  \,\,\,\,\,
 % \includegraphics[height=65mm]{corehrs_per_traj}

  
\end{frame}

\begin{frame}{The tmLQCD software suite}{[\href{https://doi.org/10.1016/j.cpc.2009.05.016}{10.1016/j.cpc.2009.05.016}, \href{https://doi.org/10.22323/1.187.0416}{10.22323/1.187.0416}, \href{https://doi.org/10.22323/1.187.0414}{10.22323/1.187.0414}, \href{https://www.github.com/etmc/tmLQCD}{gh.com/etmc/tmLQCD}]}
  \begin{columns}
    \column{0.6\textwidth}
      \begin{itemize}
        \item current HMC production code of the Extended Twisted Mass Collaboration (ETMC)
        \vspace{0.2cm}
        \item $\approx$ 150k lines (C), MPI + OpenMP, macro-based hardware specialization (intrinsics or inline assembly for SSE4, BlueGene[L,P,Q])
        \vspace{0.2cm}
        \item mainly 2 to 3 people over $\sim$ 20 years
        \begin{itemize}
          \item major contributions by another 3 to 4
          \vspace{0.1cm}
          \item small contributions by another 10 or so
        \end{itemize}
        \vspace{0.2cm}
        \item since around 2015, rely on (and extend) libraries
        \begin{itemize}
          \item QPhiX for AVX2, AVX512 (B{\'a}lint Jo{\'o} et al.) \hfill \footnotesize{\textcolor{blue}{[\href{https://doi.org/10.1007/978-3-319-46079-6_30}{10.1007/978-3-319-46079-6\_30}, \href{https://github.com/JeffersonLab/qphix}{gh.com/JeffersonLab/qphix}]}}
          \vspace{0.2cm}
        \item DD-$\alpha$AMG for MG solver on CPU \\ \footnotesize{\textcolor{blue}{[\href{https://doi.org/10.1137/130919507}{10.1137/130919507}, \href{https://doi.org/10.48550/arXiv.1307.6101}{10.48550/arXiv.1307.6101}, \href{https://doi.org/10.1103/PhysRevD.94.114509}{10.1103/PhysRevD.94.114509}, \href{https://github.com/sbacchio/DDalphaAMG}{gh.com/sbacchio/DDalphaAMG}]}}
          \vspace{0.2cm}
        \item QUDA for GPU operators and solvers (Kate Clark et al.) \footnotesize{\textcolor{blue}{[\href{https://doi.org/10.1016/j.cpc.2010.05.002}{10.1016/j.cpc.2010.05.002}, \href{https://doi.org/10.1145/2063384.2063478}{10.1145/2063384.2063478}, \href{http://dx.doi.org/10.1109/SC.2016.67}{10.1109/SC.2016.67}] } }
        \end{itemize}
        \vspace{0.15cm}
        \item long history of debates about future code for GPU machines without results (essentially lack of people power...)
      \end{itemize}
    \column{0.4\textwidth}
      \centering
      \begin{hpcablock}{}
        \centering
        \textcolor{blue}{\href{https://github.com/etmc/tmLQCD}{https://github.com/etmc/tmLQCD}}
      \end{hpcablock}
      \includegraphics[width=\textwidth]{tmlqcd_contributors.png}
  \end{columns}
\end{frame}

\begin{frame}[fragile]{Saved by the QUDA library}
  \begin{columns}
    \column{0.6\textwidth}
      \begin{itemize}
        \item First use with tmLQCD around 2015 (for observables)
        \vspace{0.20cm}
        \item Work on interface for HMC started in 2018, first running version in 2021 (motivated by QUDA performance-portability efforts)
      \end{itemize}
      \vspace{-0.3cm}
      \footnotesize
      \begin{verbatim}
  BeginExternalInverter QUDA           # equivalents of QUDA tests
    MGCoarseMuFactor = 1.0, 1.0, 60.0  # command line parameters
    MGNumberOfLevels = 3
    MGNumberOfVectors = 24, 32
    MGSetupSolver = cg
    [...]
  EndExternalInverter
  BeginMonomial CLOVERDETRATIO
    Timescale = 3
    kappa = 0.1394267
    2KappaMu = 0.000200774448
    rho = 0.0
    rho2 = 0.0018
    CSW = 1.69
    AcceptancePrecision =  1.e-21
    ForcePrecision = 1.e-18
    Name = cloverdetratio3light
    MaxSolverIterations = 500
    solver = mg
    useexternalinverter = quda         # enable QUDA pathway in solver
    usesloppyprecision = single        # driver for this monomial
  EndMonomial
      \end{verbatim}
    \column{0.4\textwidth}
      \centering
      \begin{hpcablock}{}
        \centering
        \textcolor{blue}{\href{https://github.com/lattice/quda}{https://github.com/lattice/quda}}
      \end{hpcablock}
      \includegraphics[height=90mm]{quda_contributors.png}
  \end{columns}
\end{frame}

\begin{frame}
\begin{itemize}
	\item<1-> The path integral
	\begin{flalign*}
		Z= \int DU D\psi D\bar\psi \,e^{-S_{gauge}-\bar \psi M_{deg}\psi - \bar \Psi M_{ndeg} \Psi }&&
	\end{flalign*}
	\item<2-> Integrating fermion variables
	\begin{flalign*}
		Z= \int DU  \,e^{-S_{gauge} }\det{(M_{deg})}\det{(M_{ndeg})}&&
	\end{flalign*}
	\item<3-> Even/odd Preconditioning
	\begin{flalign*}
		Z= \int DU  \,e^{-S_{gauge} }\det{\left(M_{ee}^+M_{ee}^-\right)}
		\det{(\hat Q_{+}\hat Q_{-})}
		\det{(\hat Q_h)} \det{(M_{ee}^{h})}&&
	\end{flalign*}
	\uncover<4->{
	 \begin{hpcablock}{asymmetric even-odd preconditioning}
		\begin{minipage}{\textwidth}
			\vspace{-0.25cm}
			\begin{columns}
				\column{0.6\textwidth}
%				\begin{equation*}
%				\Qpm = \gamma_5 (M_{\text{clov}} \pm i \mu_\ell \gamma_5) \; \rightarrow \; (\Qp)^\dagger = \Qm
%				\end{equation*}
%				\begin{equation*}
%				\det( \Qpm ) = \det( \Mee \pm i \mu_\ell \gamma_5 ) \cdot \det( \hQpm )
%				\end{equation*}
				\begin{equation*}
				\hQpm = \gamma_5 \left[  \Moo   - \Moe \Mee^{-1} \Meo \right]
				\end{equation*}
				\begin{equation*}
				 \Mee =1 + 2\kappa c_{SW} T_{ee} + i\tilde\mu\gamma_5
				\end{equation*}
				\column{0.4\textwidth}
				\begin{itemize}
					\item support for asym.\ precon not a given in most frameworks
					\vspace{0.2cm}
					\item issues with MG
				\end{itemize}
			\end{columns}
		\end{minipage}
	\end{hpcablock}
	}
	\uncover<5->{
	\begin{hpcablock}{even-odd preconditioning in the heavy sector: $\tau^1 \; \rightarrow$ need genuine two-flavour operators}
		\begin{equation*}
	\hat	Q_h = \gamma_5 \left[ ( \Moo + i \mu_\sigma \gamma_5 \tau^3 - \mu_\delta \tau^1 ) - \Moe (\Mee + i \mu_\sigma \gamma_5 \tau^3 - \mu_\delta \tau^1 )^{-1} \Meo \right]
		\end{equation*}
	\end{hpcablock}
	}
\end{itemize}	
\end{frame}
\begin{frame}
\begin{itemize}
	\item<1-> {\color{red} Hasenbusch trick}
	\begin{flalign*}
	Z= \int DU  \,e^{-S_{gauge} }{\color{red}\det{\left(M_{ee}^+M_{ee}^-\right)}
	\det{(\hat W_{+}\hat W_{-})}	\det{(\hat W_{+}^{-1}\hat Q_+\hat Q_-\hat W_{-}^{-1})}...\,}
	\det{(M_{ee}^{h})}
	\det{(\hat Q_h)}&&
	\end{flalign*}
	\uncover<2->{
	\begin{hpcablock}{degenerate determinant (ratios)}
		\begin{itemize}
	\item $\hWpm(\rho) = \hQpm \pm i \rho$ s.t. $\hWp(\rho) \hWm(\rho) = \hQp \hQm +\rho^2$ and clover inverse $\rho$-independent
	\item 3-4 preconditioning masses, 2-3 timescales, MG solves for smallest $\rho$ 
			\end{itemize}
	\end{hpcablock}
	}
	\item<3-> {\color{blue} Rational HMC and correction}
	\begin{multline*}
	Z= \int DU  \,e^{-S_{gauge} }\det{\left(M_{ee}^+M_{ee}^-\right)}
	\det{(\hat W_{+}\hat W_{-})}	\det{(\hat W_{+}^{-1}\hat Q_+\hat Q_-\hat W_{-}^{-1})}...\,\\
	\det{(M_{ee}^{h})}
	{\color{blue}\det{(  r_1(\hat Q_h) )}\det{(  r_2(\hat Q_h) )}...\,\det{(|\hat Q_h|{\cal R}(\hat Q_h)}}
	\end{multline*}
	\uncover<4->{\begin{hpcablock}{rational approximation partial fractions}
		\vspace{-0.25cm}
		\begin{minipage}{\textwidth}
			\begin{columns}
				\column{0.45\textwidth}
				\begin{equation*}
				\mathcal{R}(\hat Q_h^2) = \prod_{i=1}^{N} \frac{\hat Q^2_h + a_{2i-1}}{\hat Q^2_h + a_{2i}} \approx \frac{1}{\sqrt{\hat Q_h^2}}
				\end{equation*}
				\column{0.55\textwidth}
				\begin{itemize}
					\item $N \approx 10$, with $\mathcal{R}$ split across 2-3 monomials $r_i(\hat Q_h)$ on 2-3 timescales (usually 3)
					\item on CPU machines, accelerate solution of smallest shifts using DD-$\alpha$AMG {\scriptsize \textcolor{blue}{[ Bacchio, Finkenrath, 2019, \href{https://doi.org/10.1016/j.cpc.2018.10.013}{Comput.Phys.Commun. 236 (2019) 51-64}]}}
				\end{itemize}
			\end{columns}
		\end{minipage}
	\end{hpcablock}
	}
\end{itemize}	
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Diagonal operator $\det M_{ee}=\exp{\{\tr[\log M_{ee}]\}}$
		\item Off-diagonal operator with pseudofermions
		$\det{(\hat W_{+}\hat W_{-})}=\int D\phi D\phi^* \exp{\{ \phi^* (W_{+}\hat W_{-})^{-1}\phi \}}$
		\begin{flalign*}
		Z&= \int DUD\phi D\phi^*  \,e^{-S_{gauge} }{\color{blue}\det{\left(M_{ee}^+M_{ee}^-\right)}
			\det{(\hat W_{+}\hat W_{-})}}
		{\color{red}	\det{(\hat W_{+}^{-1}\hat Q_+\hat Q_-\hat W_{-}^{-1})}}...\\
		&\hspace{7cm}\det{(M_{ee}^{h})}
		{\color{mygreen}	\det{(  r_1(\hat Q_h) )}}\det{(  r_2(\hat Q_h) )}...\,
		{\color{magenta}\det{|\hat Q_h|{\cal R}(\hat Q_h)}}\\
		&= \int DU D\phi D\phi^*  \,e^{-S_{gauge} } \underbrace{\color{blue}e^{-S_{det}}
			e^{-S_{PF}}}_{\text{CLOVERDET} } \underbrace{\color{red} e^{-S_{PF}'}}_{\text{CLOERDETRATIO}}...\,
		\only<-1>{e^{-S_{det}^h}}
		\only<2->{\tikz[anchor=base, baseline] \node[fill=mygreen!40,rounded corners=2pt] (d1) {$e^{-S_{det}^h}$}; }
		\underbrace{\color{mygreen} e^{-S_{PF}^h}}_{\text{NDCLOVERRAT}}e^{-S_{PF}'^h}...\,
		\tikz[anchor=base, baseline]
		\node[fill=magenta!0,rounded corners=1pt] (d2) 	
		{$\underbrace{ \color{magenta} e^{-S_{corr}}}_{				  \text{NDCLOVERRATCOR}}$}; 
		\end{flalign*}
		\uncover<2->{\begin{tikzpicture}[overlay]
			\path[->]  (10,1) edge [in=220, out=270]  (12,0.5);
			\end{tikzpicture}}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Partition function
		\begin{flalign*}
		Z&= 
		 \int DU D\phi D\phi^*  \,e^{-S_{gauge} } \underbrace{\color{blue}e^{-S_{det}}
		e^{-S_{PF}}}_{\text{CLOVERDET} } \underbrace{\color{red} e^{-S_{PF}'}}_{\text{CLOERDETRATIO}}...\,
		\underbrace{\color{mygreen}e^{-S_{det}^h} e^{-S_{PF}^h}}_{\text{NDCLOVERRAT}}e^{-S_{PF}'^h}...\,
		\tikz[anchor=base, baseline]
			 \node[fill=magenta!0,rounded corners=1pt] (d2) 	
			 	{$\underbrace{ \color{magenta} e^{-S_{corr}}}_{				  \text{NDCLOVERRATCOR}}$}; 
		\end{flalign*}
		\item<1-> HMC
		\begin{enumerate}
			\item[1)]<2-> Heat bath pseudofermions and momenta
		\item[2)]<3-> Molecular dynamics
		\begin{align*}
			\partial_t U&=\Pi\\
			\partial_t \Pi&=-\delta S_{gauge}-
			%\only<2>
			{{\color{blue}\delta S_{det}-\delta S_{PF}}}
%			\only<3->{\underbrace{\color{blue}
%					\delta S_{det}-\delta S_{PF}}_{\text{CLOVERDET}}}
				-
			%\only<2>
			{{\color{red}\delta S_{PF}' }}
%			\only<3->{\underbrace{\color{red}
%					\delta S_{PF}'}_{\text{CLOERDETRATIO}}}
				-...-
				{\color{mygreen}\delta S^h_{det}
				-
			{\color{mygreen}\delta S^h_{PF}}}
%			\only<3->{\underbrace{\color{mygreen}\delta S^h_{PF}}_{\text{NDCLOVERRAT}}}
			-\delta S'^h_{PF}-...-
			%\only<2>
				\bcancel{{\color{magenta} \delta S_{corr}}}
%			\only<3->{\underbrace{\color{magenta}\delta S_{corr}}_{\text{NDCLOVERRATCOR}}}
		\end{align*}
%		\uncover<4->{\begin{hpcablock}{ E.g. to compute $\delta S_{PF}$}
%			\begin{equation}\nonumber
%				(\blacksquare)_x=\tr_{Dirac}\left[i\gamma_5 \sigma_{\mu\nu} Y(x)\otimes X^\dagger + i\gamma_5\sigma_{\mu\nu} X(x)\otimes Y^\dagger\right]
%			\end{equation}
%			\end{hpcablock}
%		}
		\item[3)]<4-> Accept/reject $\min\{ 1, e^{-\Delta S}\}$
		\end{enumerate}
		\item<5-> Most expensive part: the inversion of  $\hat Q_\pm$, $\hat W_{\pm}$, $\hat Q_h$ in on QUDA 
		\item<6-> $\delta S_{gauge}$ on QUDA
		\item<7-> Porting the other $\delta S_{X}$ on QUDA in
		 progress. We start from the simpler ${{\color{blue}\delta S_{det}+\delta S_{PF}}}$
		\item<8-> Porting eigenvalues measurements on QUDA in progress
		\item<9-> Gradient flow measurements on QUDA 
	\end{itemize}
	
	
\end{frame}


\begin{frame}{Hybrid CPU/GPU HMC}
  \begin{columns}
    \column{0.3\textwidth}
      \begin{itemize}
        \item gauge field and conjugate momenta in host memory
        \vspace{0.20cm}
        \item solvers and gauge term derivative on device 
        \vspace{0.20cm}
        \item need to keep track of gauge field state
        \begin{itemize}
          \item solution: tag host and device objects 
          \vspace{0.1cm}
          \item using checksum too restrictive
          \vspace{0.1cm}
          \item $\rightarrow$ simply use trajectory time (real number)
          \vspace{0.10cm}
          \item when host and device tags disagree, update device copy (optional: use thresholds)
          \vspace{0.10cm}
          \item nice side-effect: natural mechanism to track MG setup
        \end{itemize}
        \vspace{0.3cm}
        \item incremental port: need good mechanisms to identify hotspots \textit{and} their causes
      \end{itemize}
       \column{0.7\textwidth}
      \includegraphics[width=0.95\textwidth]{HMC_cpu_gpu}
  \end{columns}
\end{frame}

%\begin{frame}{The problem with profiling tools}
%  \centering
%  \includegraphics[height=87mm]{hotspots_by_level}
%  \begin{hpcablock}{}
%    \begin{itemize}
%      \item insufficient context
%      \begin{itemize}
%        \item same functions called in multiple places
%        \item unclear if profile is physically sensible or result of specific problems with certain parameter combinations or algorithms
%      \end{itemize}
%    \end{itemize}
%  \end{hpcablock}
%\end{frame}

\begin{frame}[fragile]{tmLQCD's profiler}
      \begin{verbatim}
tm_stopwatch_push(&g_timers, __func__, "");
[...]
tm_stopwatch_pop(&g_timers, 0, 0, "TM_QUDA");
      \end{verbatim}
  \begin{columns}
    \column{0.35\textwidth}
      \begin{itemize}
        \item introduced stack-based profiler into tmLQCD (and accompanying analysis scripts)
        \begin{itemize}
          \item output simply to stdout with \texttt{level0/level1/level3/...} tags
          \vspace{0.2cm}
          \item analysis parses log file (176 lines of R) and renders Rmarkdown report
          \vspace{0.2cm}
          \item Tables and plots with context and identification of call tree depth
          \vspace{0.2cm}
          \item Visualize also QUDA's finalisation profile (see backup slides) 

        \end{itemize}
      \end{itemize}
    \column{0.65\textwidth}
      {%
      \setlength{\fboxsep}{0pt}
      \setlength{\fboxrule}{0.3pt}
      \kern-0.66em\fbox{\includegraphics[width=0.18\textwidth,page=1]{n16_cA211.08.64_therm_m100}}
      \fbox{\includegraphics[width=0.18\textwidth,page=2]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=3]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=4]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=5]{n16_cA211.08.64_therm_m100}}\\ 
      \fbox{\includegraphics[width=0.18\textwidth,page=6]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=7]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=8]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=9]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=10]{n16_cA211.08.64_therm_m100}}\\
      \fbox{\includegraphics[width=0.18\textwidth,page=11]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=12]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=13]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=14]{n16_cA211.08.64_therm_m100}} 
      \fbox{\includegraphics[width=0.18\textwidth,page=15]{n16_cA211.08.64_therm_m100}}
      }
  \end{columns}
\end{frame}

\begin{frame}{tmLQCD's profiler}
  \begin{columns}
    \column{0.35\textwidth}
      \begin{itemize}
        \item combine view on physical and computational hotspots
        \vspace{0.2cm}
        \item focus on splitting of the MD Hamiltonian at this global level $\Rightarrow$
        \vspace{0.2cm}
      \end{itemize}
    \column{0.65\textwidth}
      \includegraphics[width=\textwidth]{hotspots_by_monomial}\\
      \footnotesize (profile from $64^3 \cdot 128$ physical point simulation on 16 Marconi 100 nodes)
  \end{columns}
\end{frame}

\begin{frame}{GPU-dominated parts}
  \centering
  \includegraphics[height=100mm]{gpu_dominated_monomial}
\end{frame}
  
\begin{frame}{CPU-dominated parts}
  \centering
  \includegraphics[height=100mm]{cpu_dominated_monomial}
\end{frame}

\begin{frame}{MG solver in the light sector}
  \begin{columns}
    \column{0.5\textwidth}
      \centering
      \includegraphics[width=0.9\textwidth]{mg_vs_cg} \\
      \small{Comparison between MG-preconditioned-GCR and mixed-precision CG (GPU)\\
      MG timing: two inversions + unavoidable overheads from coarse operator updates between $D$ and $D^\dagger$ inversions}
    \column{0.5\textwidth}
      \begin{hpcablock}{}
        In practice we employ
        \begin{itemize}
          \item 2 to 3 $\rho$-shifts (shifting the EO-operator) 
          \item 3-4 time scales
        \end{itemize}
        $\rightarrow$ per trajectory need to solve systems with: 
        \begin{itemize}
          \item $\rho = 0$ about $\mathcal{O}(100)$ times
          \vspace{0.2cm}
          \item $\rho \approx 0.001$ about $\mathcal{O}(100)$ times
          \vspace{0.2cm}
          \item $\rho \approx 0.01$ about $\mathcal{O}(200)$ times
          \vspace{0.2cm}
          \item $\rho \approx 0.1$ about $\mathcal{O}(400)$ times
          \vspace{0.2cm}
        \end{itemize}
        MG requires two solves in derivative and an update of the coarse operator (due to twisted mass sign change), but easily wins up to $\rho \approx am_s$.

        We employ both MG and CG to minimize total cost.
      \end{hpcablock}
  \end{columns}
\end{frame}

\begin{frame}{Multi-shift solver for the $1+1$ sector}
  \begin{columns}
    \column{0.45\textwidth}
      \centering
      \includegraphics[width=0.95\textwidth]{quda_mshift_speedup}
    \column{0.55\textwidth}
      \begin{hpcablock}{Rational Approximation Correction Term}
        \begin{itemize}
          \item $64^3 \cdot 128$ lattice
          \item CPU: 3072 cores Intel Platinum 8168 (64 Juwels nodes)
          \item GPU: 32 A100 (8 Juwels Booster nodes)
        \end{itemize}
        \vspace{0.5cm}
        \small
          \begin{tabular}{lll}
            Machine / Algorithm & HB & ACC \\
            \hline \hline \\
            (CPU) QPhiX multi-shift CG & 810 s & 550 s \\
            (CPU) DD-$\alpha$AMG accelerated multi-shift CG & 590 s & 400 s \\
            (GPU) QUDA mshift CG (double) & 145 s & 93 s \\ 
            (GPU) QUDA mshift CG (single / single) & 127 s & 79 s \\ 
            (GPU) QUDA mshift CG (single / half) & 103 s & 66 s 
          \end{tabular}
        \begin{itemize}
          \item Similar real time improvements in the derivative terms
          \item mixed-precision refinement really helps with the expensive solves (factor $\approx 1.5$)
        \end{itemize}
      \end{hpcablock}
  \end{columns}
\end{frame}


\begin{frame}[fragile]{Current state of the port and impact on simulation effort}
  \begin{columns}
    \column{0.45\textwidth}
      \centering
      \includegraphics[width=0.9\textwidth]{ndhrs_per_traj}
      %\footnotesize{(real trajectories at $M_\pi \sim 135$ MeV on $64^3 \cdot 128$ lattice)}
    \column{0.55\textwidth}
      \begin{itemize}
        \item Expect another factor of $\sim 2$ on NVIDIA (fermionic forces)
        \vspace{0.2cm}
        \item Situation on AMD to be clarified (see next slide)
        \vspace{0.2cm}
        \item Real time / GPU-hours per trajectory for different lattice sizes.
        \vspace{0.2cm}
        \item \textbf{Assuming perfect weak scaling.}
        \vspace{0.2cm}
      \item Number of trajectories achievable with typical Juwels Booster proposal (15 million coreh / 1.25 mio GPUh) and within one year (2 replicas running 24/7 for 350 days).
      \end{itemize}
  \end{columns}
  \vspace{0.3cm}
  \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{rrrrrr}
      lattice size     & real time  & 1 year              & \# GPUs & GPU-hours & 1.25 mio GPUh \\
                       & per traj.  & (2 replicas)        &         & per traj. &               \\
      \hline \\                                           
      $64^3\cdot128$   &  $1.6$ h   & \textcolor{red}{10500 traj.}         &  32     &   51      & 24000 traj.   \\
      $80^3\cdot160$   &  $1.8$ h   & \textcolor{red}{9600 traj.}          &  80     &  139      &  9000 traj.   \\
      $96^3\cdot192$   &  $2.2$ h   & 7600 traj.          & 144     &  317      &  3940 traj.   \\
      $112^3\cdot224$  &  $2.8$ h   & 5900 traj.          & 224     &  635      &  \textcolor{red}{1950 traj.}   \\    
      $128^3\cdot256$  &  $2.3$ h   & 7400 traj.          & 512     & 1158      &  \textcolor{red}{1080 traj.}   \\
      $144^3\cdot288$  &  $3.1$ h   & 5500 traj.          & 648     & 1970      &  \textcolor{red}{635 traj.}
    \end{tabular}
  \end{minipage}
\end{frame}

\begin{frame}{Current state of the port on \textcolor{blue}{Booster} / \textcolor{red}{LUMI-G} }{HMC Strong scaling}
  \begin{columns}
    \column{0.50\textwidth}
      \begin{hpcablock}{$64^3 \cdot 128$ @ $M_\pi \sim 135$ MeV on \textcolor{blue}{Booster}}
        \includegraphics[width=0.95\textwidth, page=1]{Booster_LUMIG_HMC_Scaling}
      \end{hpcablock}
    \column{0.50\textwidth}
      \begin{hpcablock}{$64^3 \cdot 128$ @ $M_\pi \sim 135$ MeV on \textcolor{red}{LUMI-G}}
        \includegraphics[width=0.95\textwidth, page=2]{Booster_LUMIG_HMC_Scaling}
      \end{hpcablock}
  \end{columns}
  \vspace{0.2cm}
  \begin{columns}
    \column{0.50\textwidth}
      \centering
      \footnotesize
      \begin{tabular}{cccc}
        \# nodes & \textcolor{blue}{Booster} & \textcolor{red}{LUMI-G} & ratio (\textcolor{red}{LUMI-G} / \textcolor{blue}{Booster}) \\
        4        & 947 s & 1400 s & 1.48 \\
        8        & 513 s &  900 s & 1.76 \\
        16       & 287 s &  720 s & 2.51 \\
        32       & 186 s &  570 s & 3.06
      \end{tabular}
    \column{0.50\textwidth}
      \begin{itemize}
        \item slow-down compared to Booster: factor 1.5 to 3
        \item (mixed) CG on 8 nodes about the same performance
        \item MG about factor 5 \textbf{slower} on LUMI-G (untuned)
      \end{itemize}
  \end{columns}
\end{frame}


%%{
%%\setbeamertemplate{background}{\BgMaterial}
%%\begin{frame}{What about performance-portability?}
%%  Single-node comparison on a $32^3 \times 64$ lattice on
%%  \begin{itemize}
%%    \item Juwels Booster ($4 \times$ A100)
%%    \item Jureca DC-MI200 ($4 \times$ AMD MI-250, \texttt{ROCm 5.2.0}, \textcolor{red}{still being fine-tuned!}).
%%  \end{itemize}
%%  \vspace{0.2cm}
%%
%%  \begin{columns}
%%    \column{0.48\textwidth}
%%      \centering
%%      \includegraphics[width=0.95\textwidth]{hip_vs_cuda_32c64} \\
%%      (full HMC run, thermalised configuration, comparable acceptance rate)
%%    \column{0.52\textwidth}
%%      \begin{tabular}{cccc}
%%        $(M_\pi / M_\pi^{\mathrm{phys}})^2$ & \cellcolor{green!50} time A100 [h] & \cellcolor{red!35}time MI250 [h] & ratio \\
%%        \hline \hline 
%%        3.75                                & 0.411         & 0.546          & 1.33 \\
%%        2.25                                & 0.478         & 0.762          & 1.59 \\
%%        1.50                                & 0.487         & 0.798          & 1.64 \\
%%        1.00                                & 0.542         & 0.975          & 1.80
%%      \end{tabular}
%%      \vspace{0.2cm}
%%      \begin{itemize}
%%        \item Time investment (for us)\footnote{major thanks to B{\'a}lint Jo{\'o} and QUDA devs for many hundreds of hours of effort which make this possible!}: 
%%        \begin{itemize}
%%          \item 2-3 hours to adjust tmLQCD build system \& compile code
%%          \item few hours with JSC admins and AMD experts to resolve a few ROCm issues
%%          \vspace{0.2cm}
%%          \item[!] get an HMC which runs on MI-250 and is \textit{at most} a factor of 2 slower even at the physical point (at least on a single node) $\rightarrow$ excellent!
%%        \end{itemize}
%%      \end{itemize}
%%  \end{columns}
%%\end{frame}

\begin{frame}{Origin of the performance difference?}{Fine-grid operator benchmark (single precision)}
  \begin{columns}
    \column{0.5\textwidth}
      \begin{hpcablock}{full node (4 A100 GPUs / 8 MI250 GCDs)}
        \includegraphics[width=0.98\textwidth]{hip_vs_cuda_dslash_bench} \\
        \begin{itemize}
          \item single node fine-grid operator performance up to factor 3 lower in region of highest relevance
          \item large local lattice: slightly faster than A100
        \end{itemize}    
      \end{hpcablock}
    \column{0.5\textwidth}
      \begin{hpcablock}{single GPU/GCD vs. 2 GPUs/GCDs (strong scaling)}
        \includegraphics[width=0.98\textwidth]{hip_vs_cuda_single_gpu_dslash_bench} \\  
        \begin{itemize}
          \item single GPU / single GCD performance (dashed, semi-transparent) vs. 2 GPUs / 2 GCDs
            \begin{itemize}
              \item exchange overhead apparently higher than for A100 even on same MCM (\texttt{HIP\_VISIBLE\_DEVICES=0,1})
            \end{itemize}
        \end{itemize}    
      \end{hpcablock}  
  \end{columns}

\end{frame}

\begin{frame}{Origin of the performance difference?}{Coarse-grid operator benchmark (single precision, 24 colours)}
  \begin{columns}
    \column{0.5\textwidth}
      \begin{hpcablock}{full node (4 A100 GPUs / 8 MI250 GCDs)}
        \includegraphics[width=0.98\textwidth]{hip_vs_cuda_mg_bench} \\
        \begin{itemize}
          \item single node coarse-grid operator performance up to factor 10 lower in region of highest relevance
        \end{itemize}    
      \end{hpcablock}
    \column{0.5\textwidth}
      \begin{hpcablock}{single GPU/GCD vs. 2 GPUs/GCDs (strong scaling)}
        \includegraphics[width=0.98\textwidth]{hip_vs_cuda_single_gpu_mg_bench} \\  
        \begin{itemize}
          \item single GPU / single GCD performance (dashed, semi-transparent) vs. 2 GPUs / 2 GCDs
            \begin{itemize}
              \item major exchange overhead even on same MCM (\texttt{HIP\_VISIBLE\_DEVICES=0,1})
            \end{itemize}
        \end{itemize}    
      \end{hpcablock}  
  \end{columns}
\end{frame}

\begin{frame}[fragile]{Pinning MPI tasks, GPUs and network interfaces on \textcolor{red}{LUMI-G}}
  \vspace{-0.4cm}
  \begin{columns}
    \small
    \column{0.4\textwidth}
    \vspace{-0.6cm}
  \begin{verbatim}
if [ $SLURM_LOCALID = "0" ]
then
    NUMA=3
    HCA=hsn0
    GPU=0
    threads=49-55,113-119
fi
if [ $SLURM_LOCALID = "1" ]
[....]
if [ $SLURM_LOCALID = "7" ]
then
    NUMA=2
    HCA=hsn3
    GPU=7
    threads=41-47,105-111
fi

[...]

CMD="env MPICH_OFI_NIC_VERBOSE=2 MPICH_OFI_NIC_POLICY=NUMA
numactl --physcpubind=${threads} --membind=$NUMA -- $exe ${args[@]}"
echo $CMD
$CMD
---> srun --cpu-bind=none ./lumi_pinwrapper.sh ${exe}
  \end{verbatim}
    \column{0.6\textwidth}
      \includegraphics[width=\textwidth]{lumig-node-architecture}
      \vspace{2.5cm}
  \end{columns}
\end{frame}

\begin{frame}{Conclusions and Outlook}
  \begin{itemize}
    \item thanks to QUDA devs, we were able to improve our energy efficiency by factor of \# $\approx 3$ already, another factor of $\approx2$ remaining  
    \vspace{0.2cm}
    \item will allow us to complete ensemble set on current \& upcoming machines
    \vspace{0.2cm}
    \item probably the end of the line for tmLQCD
    \begin{itemize}
      \item C is too limiting, data layouts too inflexible 
    \end{itemize}
    \vspace{0.2cm}
    \item time to join forces with others and / or redesign our toolset completely
    \begin{itemize}
      \item excellent performance of QUDA-MG means that it will play a role no matter what
    \end{itemize}
    \vspace{0.2cm}
    \item prepare for modular exascale machines
  \end{itemize}
  \centering
  \vspace{1.0cm}
  \textbf{Thanks for your attention!}

\end{frame}

\backupbegin

\begin{frame}{Backup}
  \centering
  \vspace{1.0cm}
  \textbf{Backup Slides}
\end{frame}

\begin{frame}{QUDA's finalisation profile (backup)}
  \begin{columns}
    \column{0.35\textwidth}
      \begin{itemize}
        \item Same analysis script also visualises QUDA's finalisation profile 
        \vspace{0.2cm}
        \item in general spend 70 to 80 \% of QUDA time in compute
        \vspace{0.2cm}
        \item host-device memory traffic is a tiny overhead (for now)
        \vspace{0.2cm}
        \item our poor decisions: too much time spent in memory allocations and frequent reinitialisations (\textit{init} and \textit{preamble})
        \vspace{0.2cm}
        \item $\rightarrow$ some potential for future improvement here
      \end{itemize}
    \column{0.65\textwidth}
      \vspace{0.1cm}
      \centering
      \includegraphics[width=0.89\textwidth]{quda_profile}
  \end{columns}
\end{frame}

\backupend

\end{document}
